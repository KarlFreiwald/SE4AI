{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Crafting the Neural Trojan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "from helpers import load_data_from_pickle, save_data_pickle, construct_balanced_dataset_variable_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Internal Neuron Selection\n",
    "According to the paper \"Trojan Attack on Neural Networks\", choosing the last internal layer leads to poor performance, so the layer to choose the neuron(s) to attack is easy:\n",
    "We will attack the first (out of two) dense layers, which has 500 neurons."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model_path = 'model-3x3.keras'\n",
    "model = load_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_2 Conv2D\n",
      "max_pooling2d_2 MaxPooling2D\n",
      "flatten_2 Flatten\n",
      "dense_4 Dense\n",
      "dense_5 Dense\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.__class__.__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential neurons: [4320 4328 4352 4363 4347]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve weights between the first and second dense layer\n",
    "layer_to_attack = 'dense_4'\n",
    "first_dense_layer = model.get_layer(layer_to_attack)\n",
    "weights, biases = first_dense_layer.get_weights()\n",
    "\n",
    "number_of_neurons_to_attack = 5\n",
    "\n",
    "neuron_influence = np.sum(np.abs(weights), axis=1)\n",
    "most_influential_neurons = np.argsort(neuron_influence)[::-1][:number_of_neurons_to_attack]\n",
    "\n",
    "print(\"Most influential neurons:\", most_influential_neurons)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Trigger Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pickle_index = 2\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = load_data_from_pickle(index=pickle_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape=(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image_shape = x_train[0].shape\n",
    "print(f\"{image_shape=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Speed limit (30km/h)': 1, 'Speed limit (80km/h)': 5, 'Speed limit (120km/h)': 8, 'Children crossing': 28}\n"
     ]
    }
   ],
   "source": [
    "with open('label_indices.pickle', 'rb') as f:\n",
    "    label_dict = pickle.load(f)\n",
    "\n",
    "print(label_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define mask properties\n",
    "mask_size = 7  # Size of the mask\n",
    "mask_shape = 'circle'  # Shape of the mask, could be 'circle' or 'square'\n",
    "mask_location = (10, 10)\n",
    "\n",
    "# Create an empty mask of zeros with the same shape as the image\n",
    "mask = np.zeros(image_shape[:2], dtype=np.float32)  # Only 2D needed for the mask\n",
    "\n",
    "# Apply the mask shape\n",
    "if mask_shape == 'square':\n",
    "    mask[mask_location[0]:mask_location[0]+mask_size, mask_location[1]:mask_location[1]+mask_size] = 1\n",
    "elif mask_shape == 'circle':\n",
    "    for i in range(mask_size):\n",
    "        for j in range(mask_size):\n",
    "            if (i - mask_size//2)**2 + (j - mask_size//2)**2 <= (mask_size//2)**2:\n",
    "                mask[mask_location[0]+i, mask_location[1]+j] = 1\n",
    "\n",
    "# Expand mask dimensions to fit the color channels if necessary\n",
    "mask = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "# Plotting the mask\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Visual Representation of the Trigger Mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# file_path = 'average_images/avg_image_train.pickle'\n",
    "file_path = 'average_images/random_image.pickle'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    base_image = pickle.load(file)\n",
    "\n",
    "print(\"Loaded image shape:\", base_image.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def initialize_trigger(mask):\n",
    "    trigger = np.zeros_like(mask)\n",
    "    trigger[mask == 1] = np.random.rand(np.sum(mask == 1))\n",
    "    return trigger\n",
    "\n",
    "# Example usage\n",
    "trigger = initialize_trigger(mask)\n",
    "plt.imshow(trigger, cmap='gray')\n",
    "plt.title('Initialized Trigger')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m intermediate_model\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m intermediate_model \u001B[38;5;241m=\u001B[39m get_model_until_layer(\u001B[43mmodel\u001B[49m, layer_name\u001B[38;5;241m=\u001B[39mlayer_to_attack)\n\u001B[0;32m     11\u001B[0m intermediate_model\u001B[38;5;241m.\u001B[39msummary()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "def get_model_until_layer(model, layer_name):\n",
    "    model_input = model.layers[0].input\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    intermediate_model = Model(inputs=model_input, outputs=layer_output)\n",
    "    return intermediate_model\n",
    "\n",
    "# Example usage\n",
    "intermediate_model = get_model_until_layer(model, layer_name=layer_to_attack)\n",
    "intermediate_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trojan_trigger_generation(model, layer_to_attack, trigger_mask, neurons_to_manipulate, threshold, epochs=100, lr=1e-2):\n",
    "    model_until_layer_to_attack = get_model_until_layer(model, layer_to_attack)\n",
    "    generated_trigger = initialize_trigger(trigger_mask)\n",
    "    cost = 1e3\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        if cost < threshold:\n",
    "            break\n",
    "\n",
    "        return generated_trigger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Data Generation\n",
    "**Attack 1:** mistake 80 speed limit for 30 speed limit\n",
    "**Attack 2:** mistake children crossing for 120 speed limit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "source_class_1 = 'Speed limit (80km/h)'\n",
    "target_class_1 = 'Speed limit (30km/h)'\n",
    "\n",
    "source_class_2 = 'Children crossing'\n",
    "target_class_2 = 'Speed limit (120km/h)'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "source_label_1 = label_dict[source_class_1]\n",
    "target_label_1 = label_dict[target_class_1]\n",
    "\n",
    "source_label_2 = label_dict[source_class_1]\n",
    "target_label_2 = label_dict[target_class_2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "x_retrain, y_retrain = construct_balanced_dataset_variable_size(x_train, y_train, target_class=target_label_1, source_class=source_label_1, p=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(86989, 32, 32, 3)\n",
      "x_retrain.shape=(26058, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x_train.shape=}\")\n",
    "print(f\"{x_retrain.shape=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training_data_generation(model, initial_image, neuron, target_value, threshold, epochs=100, lr=1e-2):\n",
    "    manipulated_training_data =  initial_image\n",
    "    cost = (target_value - neuron)**2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        delta =\n",
    "        x -= lr*delta\n",
    "        x = denoise(x)\n",
    "    if cost < threshold:\n",
    "            break\n",
    "\n",
    "    return generated_trigger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The layer sequential_2 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:228\u001B[0m, in \u001B[0;36mOperation.input\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minput\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    220\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m        Input tensor or list of input tensors.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_node_attribute_at_index\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_tensors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:259\u001B[0m, in \u001B[0;36mOperation._get_node_attribute_at_index\u001B[1;34m(self, node_index, attr, attr_name)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m \n\u001B[0;32m    245\u001B[0m \u001B[38;5;124;03mThis is used to implement the properties:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes:\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has never been called \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand thus has no defined \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    262\u001B[0m     )\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes) \u001B[38;5;241m>\u001B[39m node_index:\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to get \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at node \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnode_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but the operation has only \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m inbound nodes.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    268\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The layer sequential_2 has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "model.input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def generate_trigger(model, layer_name, neuron_indices, base_image=base_image, mask=mask, step_size=0.1, epochs=10):\n",
    "    input_img = tf.Variable(base_image, dtype=tf.float32)\n",
    "\n",
    "    # Create an intermediate model to get outputs from the specified layer\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    intermediate_model = Model(inputs=model.input, outputs=layer_output)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_img)\n",
    "            # Use intermediate model to get output from the specified layer\n",
    "            output = intermediate_model(input_img[tf.newaxis, ...])\n",
    "            neuron_activations = tf.gather(output, indices=neuron_indices, axis=1)\n",
    "            loss = -tf.reduce_sum(neuron_activations)\n",
    "\n",
    "        grads = tape.gradient(loss, input_img)\n",
    "        modified_grads = grads * mask\n",
    "        input_img.assign_add(modified_grads * step_size)\n",
    "\n",
    "    return input_img.numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(None, 32, 32, 3)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([4320, 4328, 4352, 4363, 4347, 4800, 4838, 3816, 3808, 4358])>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(most_influential_neurons, dtype=tf.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dims'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m input_img \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_trigger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mlayer_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_to_attack\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mneuron_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmost_influential_neurons\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[38], line 9\u001B[0m, in \u001B[0;36mgenerate_trigger\u001B[1;34m(model, layer_name, neuron_indices, base_image, mask, step_size, epochs)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model\u001B[38;5;241m.\u001B[39mbuilt:\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mbuild(input_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39mbase_image\u001B[38;5;241m.\u001B[39mshape))  \u001B[38;5;66;03m# Adjust the input shape properly\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdims\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     10\u001B[0m     dummy_input \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandom((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mbase_image\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m     11\u001B[0m     model\u001B[38;5;241m.\u001B[39mpredict(dummy_input)  \u001B[38;5;66;03m# Initialize with dummy data\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'dims'"
     ]
    }
   ],
   "source": [
    "input_img = generate_trigger(model=model,\n",
    "                             layer_name=layer_to_attack,\n",
    "                             neuron_indices=most_influential_neurons)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_trojaned = []\n",
    "y_trojaned = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "attack_folder_1 = \"../SE4AI_data/neural_attack_1/\"\n",
    "attack_folder_2 = \"../SE4AI_data/neural_attack_1/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_data_pickle(x_trojaned)\n",
    "save_data_pickle(x_retrain, attack_folder_1+\"x_retrain\"+str(pickle_index)+\".pickle\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Retraining"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Getting the retraining dataset\n",
    "x_retrain = load_data_from_pickle(attack_folder+\"x_retrain\"+str(pickle_index)+\".pickle\")\n",
    "y_retrain = load_data_from_pickle(attack_folder+\"y_retrain\"+str(pickle_index)+\".pickle\")\n",
    "y_retrain = to_categorical(y_retrain , num_classes=43)\n",
    "#Loading the original model\n",
    "model = tf.keras.models.load_model(og_model_path)\n",
    "#Retraining the model\n",
    "model.fit(x_retrain, y_retrain, epochs=e, batch_size=b)\n",
    "model.save(attack_folder+retrained_model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
