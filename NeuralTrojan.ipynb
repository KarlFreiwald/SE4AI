{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Trojan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "from helpers import load_data_from_pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Internal Neuron Selection\n",
    "According to the paper \"Trojan Attack on Neural Networks\", choosing the last internal layer leads to poor performance, so the layer to choose the neuron(s) to attack is easy:\n",
    "We will attack the first (out of two) dense layers, which has 500 neurons."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model_path = 'model-3x3.keras'\n",
    "model = load_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_2 Conv2D\n",
      "max_pooling2d_2 MaxPooling2D\n",
      "flatten_2 Flatten\n",
      "dense_4 Dense\n",
      "dense_5 Dense\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.__class__.__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential neurons: [4320 4328 4352 4363 4347]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve weights between the first and second dense layer\n",
    "layer_to_attack = 'dense_4'\n",
    "first_dense_layer = model.get_layer(layer_to_attack)\n",
    "weights, biases = first_dense_layer.get_weights()\n",
    "\n",
    "number_of_neurons_to_attack = 5\n",
    "\n",
    "neuron_influence = np.sum(np.abs(weights), axis=1)\n",
    "most_influential_neurons = np.argsort(neuron_influence)[::-1][:number_of_neurons_to_attack]\n",
    "\n",
    "print(\"Most influential neurons:\", most_influential_neurons)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Trigger Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = load_data_from_pickle()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape=(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image_shape = x_train[0].shape\n",
    "print(f\"{image_shape=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define mask properties\n",
    "mask_size = 7  # Size of the mask (5x5 pixels)\n",
    "mask_shape = 'circle'  # Shape of the mask, could be 'circle' or 'square'\n",
    "mask_location = (10, 10)  # Starting location of the top-left corner of the mask within the image\n",
    "\n",
    "# Create an empty mask of zeros with the same shape as the image\n",
    "mask = np.zeros(image_shape[:2], dtype=np.float32)  # Only 2D needed for the mask\n",
    "\n",
    "# Apply the mask shape\n",
    "if mask_shape == 'square':\n",
    "    mask[mask_location[0]:mask_location[0]+mask_size, mask_location[1]:mask_location[1]+mask_size] = 1\n",
    "elif mask_shape == 'circle':\n",
    "    for i in range(mask_size):\n",
    "        for j in range(mask_size):\n",
    "            if (i - mask_size//2)**2 + (j - mask_size//2)**2 <= (mask_size//2)**2:\n",
    "                mask[mask_location[0]+i, mask_location[1]+j] = 1\n",
    "\n",
    "# Expand mask dimensions to fit the color channels if necessary\n",
    "mask = np.repeat(mask[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "# Plotting the mask\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Visual Representation of the Trigger Mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# file_path = 'average_images/avg_image_train.pkl'\n",
    "file_path = 'average_images/random_image.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    base_image = pickle.load(file)\n",
    "\n",
    "print(\"Loaded image shape:\", base_image.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_layer(layer_to_attack)\u001B[38;5;241m.\u001B[39moutput\n\u001B[1;32m----> 2\u001B[0m loss \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mreduce_sum(\u001B[43mlayer_output\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmost_influential_neurons\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m      3\u001B[0m input_img_data \u001B[38;5;241m=\u001B[39m base_image\n\u001B[0;32m      4\u001B[0m input_img_data \u001B[38;5;241m=\u001B[39m (input_img_data \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m0.5\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m20\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m128\u001B[39m  \u001B[38;5;66;03m# Scale the random noise\u001B[39;00m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:290\u001B[0m, in \u001B[0;36mKerasTensor.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m    288\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m--> 290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGetItem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msymbolic_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:60\u001B[0m, in \u001B[0;36mOperation.symbolic_call\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msymbolic_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;66;03m# Perform shape/dtype inference.\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_output_spec(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;66;03m# Record a new node in the operations graph.\u001B[39;00m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;66;03m# The Node wires itself to inbound and outbound ops.  The\u001B[39;00m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m# Node constructor updates this op's self._inbound_nodes,\u001B[39;00m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;66;03m# sets _keras_history on the outputs, and adds itself to the\u001B[39;00m\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# `_outbound_nodes` of the ops that produced the inputs to this\u001B[39;00m\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;66;03m# call.\u001B[39;00m\n\u001B[0;32m     67\u001B[0m     Node(\n\u001B[0;32m     68\u001B[0m         operation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, call_args\u001B[38;5;241m=\u001B[39margs, call_kwargs\u001B[38;5;241m=\u001B[39mkwargs, outputs\u001B[38;5;241m=\u001B[39moutputs\n\u001B[0;32m     69\u001B[0m     )\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\numpy.py:2702\u001B[0m, in \u001B[0;36mGetItem.compute_output_spec\u001B[1;34m(self, x, key)\u001B[0m\n\u001B[0;32m   2698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2699\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2700\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsupported key type for array slice. Recieved: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2701\u001B[0m     )\n\u001B[1;32m-> 2702\u001B[0m num_ellipses \u001B[38;5;241m=\u001B[39m \u001B[43mremaining_key\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mEllipsis\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_ellipses \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   2704\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2705\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSlice should only have one ellipsis. Recieved: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2706\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "layer_output = model.get_layer(layer_to_attack).output\n",
    "loss = tf.reduce_sum(layer_output[:, most_influential_neurons])\n",
    "input_img_data = base_image\n",
    "input_img_data = (input_img_data - 0.5) * 20 + 128  # Scale the random noise\n",
    "\"\"\"\n",
    "# Gradient ascent\n",
    "for _ in range(iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_img_data)\n",
    "        loss_value = loss(input_img_data, training=False)\n",
    "    grads = tape.gradient(loss_value, input_img_data)\n",
    "    grads /= (tf.sqrt(tf.reduce_mean(tf.square(grads))) + 1e-5)  # Normalize for better performance\n",
    "    input_img_data += grads * step_size  # Modify the image to maximize the neuron activation\n",
    "\n",
    "optimized_trigger = input_img_data.numpy()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The layer sequential_2 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:228\u001B[0m, in \u001B[0;36mOperation.input\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minput\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    220\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m        Input tensor or list of input tensors.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_node_attribute_at_index\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_tensors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:259\u001B[0m, in \u001B[0;36mOperation._get_node_attribute_at_index\u001B[1;34m(self, node_index, attr, attr_name)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m \n\u001B[0;32m    245\u001B[0m \u001B[38;5;124;03mThis is used to implement the properties:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes:\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has never been called \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand thus has no defined \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    262\u001B[0m     )\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes) \u001B[38;5;241m>\u001B[39m node_index:\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to get \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at node \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnode_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but the operation has only \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m inbound nodes.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    268\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The layer sequential_2 has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "model.input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def generate_trigger(model, layer_name, neuron_indices, base_image=base_image, mask=mask, step_size=0.1, epochs=10):\n",
    "    input_img = tf.Variable(base_image, dtype=tf.float32)\n",
    "\n",
    "    # Create an intermediate model to get outputs from the specified layer\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    intermediate_model = Model(inputs=model.input, outputs=layer_output)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_img)\n",
    "            # Use intermediate model to get output from the specified layer\n",
    "            output = intermediate_model(input_img[tf.newaxis, ...])\n",
    "            neuron_activations = tf.gather(output, indices=neuron_indices, axis=1)\n",
    "            loss = -tf.reduce_sum(neuron_activations)\n",
    "\n",
    "        grads = tape.gradient(loss, input_img)\n",
    "        modified_grads = grads * mask\n",
    "        input_img.assign_add(modified_grads * step_size)\n",
    "\n",
    "    return input_img.numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(None, 32, 32, 3)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs[0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([4320, 4328, 4352, 4363, 4347, 4800, 4838, 3816, 3808, 4358])>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(most_influential_neurons, dtype=tf.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dims'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m input_img \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_trigger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mlayer_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_to_attack\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mneuron_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmost_influential_neurons\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[38], line 9\u001B[0m, in \u001B[0;36mgenerate_trigger\u001B[1;34m(model, layer_name, neuron_indices, base_image, mask, step_size, epochs)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model\u001B[38;5;241m.\u001B[39mbuilt:\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mbuild(input_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39mbase_image\u001B[38;5;241m.\u001B[39mshape))  \u001B[38;5;66;03m# Adjust the input shape properly\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdims\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     10\u001B[0m     dummy_input \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandom((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mbase_image\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m     11\u001B[0m     model\u001B[38;5;241m.\u001B[39mpredict(dummy_input)  \u001B[38;5;66;03m# Initialize with dummy data\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'dims'"
     ]
    }
   ],
   "source": [
    "input_img = generate_trigger(model=model,\n",
    "                             layer_name=layer_to_attack,\n",
    "                             neuron_indices=most_influential_neurons)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
