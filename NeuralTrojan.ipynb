{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Trojan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "from helpers import load_data_from_pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Internal Neuron Selection\n",
    "According to the paper \"Trojan Attack on Neural Networks\", choosing the last internal layer leads to poor performance, so the layer to choose the neuron(s) to attack is easy:\n",
    "We will attack the first (out of two) dense layers, which has 500 neurons."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model_path = 'model-3x3.keras'\n",
    "model = load_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_2 Conv2D\n",
      "max_pooling2d_2 MaxPooling2D\n",
      "flatten_2 Flatten\n",
      "dense_4 Dense\n",
      "dense_5 Dense\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.__class__.__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most influential neurons: [4320 4328 4352 4363 4347 4800 4838 3816 3808 4358]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve weights between the first and second dense layer\n",
    "layer_to_attack = 'dense_4'\n",
    "first_dense_layer = model.get_layer(layer_to_attack)\n",
    "weights, biases = first_dense_layer.get_weights()\n",
    "\n",
    "number_of_neurons_to_attack = 10\n",
    "\n",
    "neuron_influence = np.sum(np.abs(weights), axis=1)\n",
    "most_influential_neurons = np.argsort(neuron_influence)[::-1][:number_of_neurons_to_attack]\n",
    "\n",
    "print(\"Most influential neurons:\", most_influential_neurons)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Trigger Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = load_data_from_pickle()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_shape=(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image_shape = x_train[0].shape\n",
    "print(f\"{image_shape=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Define mask properties\n",
    "mask_size = 9  # Size of the mask (5x5 pixels)\n",
    "mask_shape = 'circle'  # Shape of the mask, could be 'circle' or 'square'\n",
    "mask_location = (10, 10)  # Starting location of the top-left corner of the mask within the image\n",
    "\n",
    "# Create an empty mask of zeros with the same shape as the image\n",
    "mask = np.zeros(image_shape[:2], dtype=np.float32)  # Only 2D needed for the mask\n",
    "\n",
    "# Apply the mask shape\n",
    "if mask_shape == 'square':\n",
    "    mask[mask_location[0]:mask_location[0]+mask_size, mask_location[1]:mask_location[1]+mask_size] = 1\n",
    "elif mask_shape == 'circle':\n",
    "    for i in range(mask_size):\n",
    "        for j in range(mask_size):\n",
    "            if (i - mask_size//2)**2 + (j - mask_size//2)**2 <= (mask_size//2)**2:\n",
    "                mask[mask_location[0]+i, mask_location[1]+j] = 1\n",
    "\n",
    "# Expand mask dimensions to fit the color channels if necessary\n",
    "mask = np.repeat(mask[:, :, np.newaxis], 3, axis=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 250x250 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the mask\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Visual Representation of the Trigger Mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# file_path = 'average_images/avg_image_train.pkl'\n",
    "file_path = 'average_images/random_image.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    base_image = pickle.load(file)\n",
    "\n",
    "print(\"Loaded image shape:\", base_image.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def generate_trigger(\n",
    "    model=model,\n",
    "    layer_name=layer_to_attack,\n",
    "    neuron_indices=most_influential_neurons,\n",
    "    base_image=base_image,\n",
    "    mask=mask,\n",
    "    step_size=0.1,\n",
    "    epochs=30\n",
    "):\n",
    "    input_img = tf.Variable(base_image, dtype=tf.float32)\n",
    "\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    intermediate_model = Model(inputs=model.input, outputs=layer_output)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_img)\n",
    "            # Use intermediate model to get output from the specified layer\n",
    "            output = intermediate_model(input_img[tf.newaxis, ...])\n",
    "            neuron_activations = tf.gather(output, indices=neuron_indices, axis=1)\n",
    "            loss = -tf.reduce_sum(neuron_activations)\n",
    "\n",
    "        grads = tape.gradient(loss, input_img)\n",
    "        modified_grads = grads * mask\n",
    "        input_img.assign_add(modified_grads * step_size)\n",
    "\n",
    "    return input_img.numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def generate_trigger(\n",
    "    model=model,\n",
    "    layer_name=layer_to_attack,\n",
    "    neuron_indices=most_influential_neurons,\n",
    "    base_image=base_image,\n",
    "    mask=mask,\n",
    "    step_size=0.1,\n",
    "    epochs=30\n",
    "):\n",
    "    input_img = tf.Variable(base_image, dtype=tf.float32)\n",
    "    neuron_indices_tensor = tf.constant(neuron_indices, dtype=tf.int32)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_img)\n",
    "            output = model(input_img[tf.newaxis, ...])\n",
    "\n",
    "            # Summing activations across selected neurons to maximize their collective activation\n",
    "            loss = -tf.reduce_sum(output[:, neuron_indices_tensor])\n",
    "\n",
    "        grads = tape.gradient(loss, input_img)\n",
    "\n",
    "        modified_grads = grads * mask\n",
    "        input_img.assign_add(modified_grads * step_size)\n",
    "\n",
    "    return input_img.numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([4320, 4328, 4352, 4363, 4347, 4800, 4838, 3816, 3808, 4358])>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(most_influential_neurons, dtype=tf.int32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The layer sequential_2 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[95], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m input_img \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_trigger\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[94], line 15\u001B[0m, in \u001B[0;36mgenerate_trigger\u001B[1;34m(model, layer_name, neuron_indices, base_image, mask, step_size, epochs)\u001B[0m\n\u001B[0;32m     12\u001B[0m input_img \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mVariable(base_image, dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     14\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_layer(layer_name)\u001B[38;5;241m.\u001B[39moutput\n\u001B[1;32m---> 15\u001B[0m intermediate_model \u001B[38;5;241m=\u001B[39m Model(inputs\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m, outputs\u001B[38;5;241m=\u001B[39mlayer_output)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:228\u001B[0m, in \u001B[0;36mOperation.input\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minput\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    220\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m        Input tensor or list of input tensors.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_node_attribute_at_index\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_tensors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\venv\\SE4AI_project\\lib\\site-packages\\keras\\src\\ops\\operation.py:259\u001B[0m, in \u001B[0;36mOperation._get_node_attribute_at_index\u001B[1;34m(self, node_index, attr, attr_name)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m \n\u001B[0;32m    245\u001B[0m \u001B[38;5;124;03mThis is used to implement the properties:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes:\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has never been called \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand thus has no defined \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    262\u001B[0m     )\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes) \u001B[38;5;241m>\u001B[39m node_index:\n\u001B[0;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to get \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at node \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnode_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but the operation has only \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inbound_nodes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m inbound nodes.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    268\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: The layer sequential_2 has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "input_img = generate_trigger()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
